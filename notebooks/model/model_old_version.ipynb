{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('gs://wmt-mlp-p-intlctlg-export-bucket/AE/GenAI/Experiment/train_v9rqX0R.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Enhanced data preprocessing with advanced feature engineering\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original data\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Handle missing values with more sophisticated approaches\n",
    "    df['Item_Weight'] = df['Item_Weight'].fillna(df.groupby('Item_Type')['Item_Weight'].transform('median'))\n",
    "    \n",
    "    # Fix the Item_Visibility replacement\n",
    "    mask = df['Item_Visibility'] == 0\n",
    "    df.loc[mask, 'Item_Visibility'] = df.groupby('Item_Type')['Item_Visibility'].transform('median')\n",
    "    \n",
    "    # Fill missing Outlet_Size based on Outlet_Type and Location\n",
    "    outlet_size_mapping = {\n",
    "        'Grocery Store': 'Small',\n",
    "        'Supermarket Type1': 'Small',\n",
    "        'Supermarket Type2': 'Medium',\n",
    "        'Supermarket Type3': 'Medium'\n",
    "    }\n",
    "    \n",
    "    for outlet_type, size in outlet_size_mapping.items():\n",
    "        df.loc[(df['Outlet_Type'] == outlet_type) & df['Outlet_Size'].isnull(), 'Outlet_Size'] = size\n",
    "    \n",
    "    # Feature Engineering\n",
    "    # 1. Item Category Features\n",
    "    df['Item_Category'] = df['Item_Identifier'].str[:2]\n",
    "    df['Is_Food'] = df['Item_Category'].isin(['FD', 'DR']).astype(int)\n",
    "    \n",
    "    # 2. Time-based Features\n",
    "    current_year = 2025\n",
    "    df['Outlet_Age'] = current_year - df['Outlet_Establishment_Year']\n",
    "    df['Outlet_Age_Squared'] = df['Outlet_Age'] ** 2\n",
    "    df['Is_Old_Store'] = (df['Outlet_Age'] > df['Outlet_Age'].median()).astype(int)\n",
    "    \n",
    "    # 3. Price-based Features\n",
    "    df['Price_per_Weight'] = df['Item_MRP'] / df['Item_Weight']\n",
    "    #df['Price_Level'] = pd.qcut(df['Item_MRP'], q=4, labels=[1,2,3,4])\n",
    "    df['Price_Relative_To_Type'] = df['Item_MRP'] / df.groupby('Item_Type')['Item_MRP'].transform('mean')\n",
    "    \n",
    "    # 4. Visibility Features\n",
    "    df['Visibility_MRP'] = df['Item_Visibility'] * df['Item_MRP']\n",
    "    df['Visibility_Type_Mean'] = df.groupby('Item_Type')['Item_Visibility'].transform('mean')\n",
    "    df['Visibility_Ratio'] = df['Item_Visibility'] / (df['Visibility_Type_Mean'] + 1e-6)  # Avoid division by zero\n",
    "    \n",
    "    # 5. Store Features\n",
    "    df['Store_Age_Size'] = df['Outlet_Age'] * pd.Categorical(df['Outlet_Size']).codes\n",
    "    df['Items_per_Store'] = df.groupby('Outlet_Identifier')['Item_Identifier'].transform('count')\n",
    "    df['Store_Avg_Price'] = df.groupby('Outlet_Identifier')['Item_MRP'].transform('mean')\n",
    "    \n",
    "    # 6. Log Transformations for Skewed Features\n",
    "    numeric_features = ['Item_Weight', 'Item_Visibility', 'Price_per_Weight', 'Visibility_MRP']\n",
    "    for feature in numeric_features:\n",
    "        df[feature] = np.log1p(df[feature])\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in cat_cols:\n",
    "        oe = OrdinalEncoder()\n",
    "        df[col] = oe.fit_transform(df[[col]])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_cols = []\n",
    "for col in data.columns:\n",
    "    if pd.api.types.is_string_dtype(data[col]):\n",
    "        string_cols.append(col)\n",
    "string_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = data.drop('Item_Outlet_Sales',axis=1)\n",
    "y = data['Item_Outlet_Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_top_features_model(X, y, n_features=20):\n",
    "    \"\"\"\n",
    "    Train XGBoost model with feature selection based on importance thresholds\n",
    "    and hyperparameter tuning using Mean Squared Error (MSE) as the evaluation metric\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pandas DataFrame\n",
    "        Input features\n",
    "    y : pandas Series\n",
    "        Target variable\n",
    "    n_features : int\n",
    "        number of features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        best_model, selected_features, feature_importance, best_score\n",
    "    \"\"\"\n",
    "    # Import necessary libraries and metrics\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn.model_selection import ParameterSampler\n",
    "    import numpy as np\n",
    "    \n",
    "    # Split data into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initial feature importance calculation using XGBoost\n",
    "    initial_model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "    initial_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': initial_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Calculate cumulative importance and normalized importance\n",
    "    total_importance = feature_importance['importance'].sum()\n",
    "    feature_importance['importance_normalized'] = feature_importance['importance'] / total_importance\n",
    "    feature_importance['cumulative_importance'] = feature_importance['importance_normalized'].cumsum()\n",
    "    \n",
    "    print(\"Feature importance distribution:\")\n",
    "    print(feature_importance[['feature', 'importance_normalized', 'cumulative_importance']].head(10))\n",
    "    \n",
    "    # Define importance thresholds to try instead of fixed feature counts\n",
    "    # These thresholds represent the minimum normalized importance for a feature to be included\n",
    "    importance_thresholds = [0.0001, 0.001, 0.005, 0.01, 0.02, 0.03]\n",
    "    cumulative_thresholds = [0.75, 0.80, 0.85, 0.90, 0.95, 0.99]\n",
    "    \n",
    "    # For MSE, lower is better, so initialize with infinity\n",
    "    best_val_mse = float('inf')\n",
    "    best_params = None\n",
    "    best_features = None\n",
    "    best_model = None\n",
    "    best_threshold_type = None\n",
    "    best_threshold_value = None\n",
    "    \n",
    "    # Define hyperparameter search space for XGBoost\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'max_depth': [3, 5, 7, 9],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'gamma': [0, 0.1, 0.2]\n",
    "    }\n",
    "    \n",
    "    # Sample a limited number of hyperparameter combinations\n",
    "    param_list = list(ParameterSampler(\n",
    "        param_grid, \n",
    "        n_iter=10,\n",
    "        random_state=42\n",
    "    ))\n",
    "    \n",
    "    print(\"Starting importance threshold and hyperparameter grid search on validation set...\")\n",
    "    \n",
    "    # Try feature selection based on minimum importance threshold\n",
    "    for threshold in importance_thresholds:\n",
    "        # Select features with importance above threshold\n",
    "        selected_features = feature_importance[feature_importance['importance_normalized'] > threshold]['feature'].tolist()\n",
    "        \n",
    "        if len(selected_features) < 5:  # Ensure we have at least 5 features\n",
    "            continue\n",
    "            \n",
    "        print(f\"Testing minimum importance threshold {threshold}: selected {len(selected_features)} features\")\n",
    "        \n",
    "        # Prepare data with selected features\n",
    "        X_train_selected = X_train[selected_features]\n",
    "        X_val_selected = X_val[selected_features]\n",
    "        \n",
    "        # Try different hyperparameter combinations\n",
    "        for params in param_list:\n",
    "            # Create and train XGBoost model with current hyperparameters\n",
    "            model = XGBRegressor(random_state=42, **params)\n",
    "            model.fit(X_train_selected, y_train)\n",
    "            \n",
    "            # Evaluate on validation set using MSE\n",
    "            val_predictions = model.predict(X_val_selected)\n",
    "            val_mse = mean_squared_error(y_val, val_predictions)\n",
    "            \n",
    "            # Update best configuration if this performs better\n",
    "            if val_mse < best_val_mse:\n",
    "                best_val_mse = val_mse\n",
    "                best_params = params\n",
    "                best_features = selected_features\n",
    "                best_model = model\n",
    "                best_threshold_type = \"min_importance\"\n",
    "                best_threshold_value = threshold\n",
    "                \n",
    "                print(f\"New best: min_importance {threshold}, {len(selected_features)} features, validation MSE: {val_mse:.4f}\")\n",
    "    \n",
    "    # Try feature selection based on cumulative importance threshold\n",
    "    for threshold in cumulative_thresholds:\n",
    "        # Select features up to the cumulative importance threshold\n",
    "        selected_features = feature_importance[feature_importance['cumulative_importance'] <= threshold]['feature'].tolist()\n",
    "        \n",
    "        if len(selected_features) < 5:  # Ensure we have at least 5 features\n",
    "            continue\n",
    "            \n",
    "        print(f\"Testing cumulative importance threshold {threshold}: selected {len(selected_features)} features\")\n",
    "        \n",
    "        # Prepare data with selected features\n",
    "        X_train_selected = X_train[selected_features]\n",
    "        X_val_selected = X_val[selected_features]\n",
    "        \n",
    "        # Try different hyperparameter combinations\n",
    "        for params in param_list:\n",
    "            # Create and train XGBoost model with current hyperparameters\n",
    "            model = XGBRegressor(random_state=42, **params)\n",
    "            model.fit(X_train_selected, y_train)\n",
    "            \n",
    "            # Evaluate on validation set using MSE\n",
    "            val_predictions = model.predict(X_val_selected)\n",
    "            val_mse = mean_squared_error(y_val, val_predictions)\n",
    "            \n",
    "            # Update best configuration if this performs better\n",
    "            if val_mse < best_val_mse:\n",
    "                best_val_mse = val_mse\n",
    "                best_params = params\n",
    "                best_features = selected_features\n",
    "                best_model = model\n",
    "                best_threshold_type = \"cumulative_importance\"\n",
    "                best_threshold_value = threshold\n",
    "                \n",
    "                print(f\"New best: cumulative_importance {threshold}, {len(selected_features)} features, validation MSE: {val_mse:.4f}\")\n",
    "    \n",
    "    # Get final feature importance from the best model\n",
    "    final_importance = pd.DataFrame({\n",
    "        'feature': best_features,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nFinal Model Selected:\")\n",
    "    print(f\"Feature selection method: {best_threshold_type} with threshold {best_threshold_value}\")\n",
    "    print(f\"Number of features: {len(best_features)}\")\n",
    "    print(f\"Validation MSE: {best_val_mse:.4f}\")\n",
    "    print(\"\\nBest Parameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(final_importance.head(10))\n",
    "    \n",
    "    # Return best model, features, importance, and MSE score\n",
    "    return best_model, best_features, feature_importance, best_val_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance distribution:\n",
      "                      feature  importance_normalized  cumulative_importance\n",
      "10                Outlet_Type               0.604685               0.604685\n",
      "5                    Item_MRP               0.106427               0.711112\n",
      "7   Outlet_Establishment_Year               0.086716               0.797828\n",
      "17     Price_Relative_To_Type               0.026675               0.824503\n",
      "9        Outlet_Location_Type               0.016469               0.840972\n",
      "22            Items_per_Store               0.014953               0.855925\n",
      "21             Store_Age_Size               0.014019               0.869944\n",
      "16           Price_per_Weight               0.013288               0.883233\n",
      "20           Visibility_Ratio               0.012862               0.896094\n",
      "19       Visibility_Type_Mean               0.012257               0.908351\n",
      "Starting importance threshold and hyperparameter grid search on validation set...\n",
      "Testing minimum importance threshold 0.0001: selected 19 features\n",
      "New best: min_importance 0.0001, 19 features, validation MSE: 1157451.4411\n",
      "New best: min_importance 0.0001, 19 features, validation MSE: 1072583.8698\n",
      "New best: min_importance 0.0001, 19 features, validation MSE: 1065190.1427\n",
      "Testing minimum importance threshold 0.001: selected 19 features\n",
      "Testing minimum importance threshold 0.005: selected 19 features\n",
      "Testing minimum importance threshold 0.01: selected 15 features\n",
      "New best: min_importance 0.01, 15 features, validation MSE: 1057532.9008\n",
      "Testing cumulative importance threshold 0.85: selected 5 features\n",
      "Testing cumulative importance threshold 0.9: selected 9 features\n",
      "New best: cumulative_importance 0.9, 9 features, validation MSE: 1056177.8458\n",
      "Testing cumulative importance threshold 0.95: selected 13 features\n",
      "Testing cumulative importance threshold 0.99: selected 17 features\n",
      "\n",
      "Final Model Selected:\n",
      "Feature selection method: cumulative_importance with threshold 0.9\n",
      "Number of features: 9\n",
      "Validation MSE: 1056177.8458\n",
      "\n",
      "Best Parameters:\n",
      "subsample: 0.8\n",
      "n_estimators: 200\n",
      "min_child_weight: 5\n",
      "max_depth: 3\n",
      "learning_rate: 0.1\n",
      "gamma: 0\n",
      "colsample_bytree: 1.0\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                     feature  importance\n",
      "0                Outlet_Type    0.588746\n",
      "1                   Item_MRP    0.209601\n",
      "5            Items_per_Store    0.049972\n",
      "3     Price_Relative_To_Type    0.040875\n",
      "4       Outlet_Location_Type    0.030965\n",
      "6             Store_Age_Size    0.024517\n",
      "7           Price_per_Weight    0.020393\n",
      "8           Visibility_Ratio    0.018035\n",
      "2  Outlet_Establishment_Year    0.016894\n",
      "CPU times: user 4min 43s, sys: 791 ms, total: 4min 44s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_model, features, feature_importance, mean_r2_score= train_top_features_model(X, y, n_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1056177.8458175422"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Outlet_Type',\n",
       " 'Item_MRP',\n",
       " 'Outlet_Establishment_Year',\n",
       " 'Price_Relative_To_Type',\n",
       " 'Outlet_Location_Type',\n",
       " 'Items_per_Store',\n",
       " 'Store_Age_Size',\n",
       " 'Price_per_Weight',\n",
       " 'Visibility_Ratio']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('gs://wmt-mlp-p-intlctlg-export-bucket/AE/GenAI/Experiment/test_AbJTz2l.csv')\n",
    "test_data2 = test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data2 = preprocess_data(test_data2)\n",
    "test_data2 = test_data2[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_result = final_model.predict(test_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data['Item_Outlet_Sales'] = test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = test_data[['Item_Identifier', 'Outlet_Identifier', 'Item_Outlet_Sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data.to_csv('gs://wmt-mlp-p-intlctlg-export-bucket/AE/GenAI/Experiment/submissionv9.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
